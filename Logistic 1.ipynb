{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d5997e-4872-4fb6-8d13-7da0ee25ebc0",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5904c18-b0a7-46e9-9d72-47ce35f0331f",
   "metadata": {},
   "source": [
    "Logistic Regression is used to solve classification problems of categorical variables.\n",
    "Let’s solve a problem using logistic regression. We have a dataset containing 2 variables Play Hours and Pass/Fail. If play hour is 4, student will pass. If play hour is 6, student will fail. This is a train dataset. We have to train a model using this dataset and create a hypothesis. The work of hypothesis is to predict the Pass/Fail using new play hours data i.e. Play Hours is the input and Pass/Fail is the output. The output variable (Pass/Fail) is a categorical variable that means the dataset is binary.\n",
    "In the graph, the x – axis is of play hours (input) and on y – axis is the output pass/fail.\n",
    "When play hour is 0, 1, 2 the output will be pass i.e. 1. If input is 5,6,7,……n the output will be fail i.e. 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e9cab-3a63-4bea-81e7-5fd9e16cfe6b",
   "metadata": {},
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a70a5-5df0-4241-a1e1-699c041e6346",
   "metadata": {},
   "source": [
    "For the Logistic regression, the cost function will be almost same as the cost function of linear regression and is denoted by J(θ_0,θ_1 )  and given by:\n",
    "J(θ_0,θ_1 )=1/n ∑_(i=1)^n▒(〖y_i-h〗_θ (x)_i )^2 \n",
    "But here;\n",
    "h_θ (x)=1/(1+e^(-〖(θ〗_0+θ_1 x_1)) )\n",
    "Which is a non-convex function.\n",
    "We know in linear regression the cost function form is of gradient descent type which is a convex function. As a part of convex function we have to come close to the global minima of gradient descent curve.\n",
    "But in non-convex function the graph formed will have multiple curves because of the sigmoid function as shown here. These curves are known as local minima. If there is any gradient descent in local minima, it will stuck there as slope of local minima is always 0. If slope is 0 the θ value will not update and will not be able to reach global minima. \n",
    "So we will use different cost function format for logistic regression which is known as log loss. Log loss is given by;\n",
    "J(θ_0,θ_1 )=-ylog(h_θ (x))-(1-y)log(1-h_θ (x))\n",
    "If y=1 then; J(θ_0,θ_1 )=-log(h_θ (x))\n",
    "If y=0 then; J(θ_0,θ_1 )=log(1-h_θ (x))\n",
    "So we will get 2 situations;\n",
    "J(θ_0,θ_1 ){█(-log(h_θ (x)),                 if y=1@-log(1-h_θ (x)),if y=0)┤\n",
    "This is a convex equation. If we use this as formula for cost function we will see we get a convex curve in the graph of cost function. If we keep plotting this formula as cost function we will get a gradient descent type curve in the graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645c50d-ee62-4385-9c89-dd2abef027c6",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf1bf7-e1de-43a6-b2d2-e56e9acd02a4",
   "metadata": {},
   "source": [
    "In ridge regression we have seen that we use L2 regularization and in lasso regression we use L1 regularization and in elastic net regression we combine both L1 and L2 regularization. Same regularization parameters are also used in logistic regression.\n",
    "L2 REGULARIZATION\n",
    "Used for reducing overfitting and is given by:\n",
    "J(θ_0,θ_1 )=-ylog(h_θ (x))-(1-y)log(1-h_θ (x))+λ∑_(i=1)^n▒(Slope)^2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fac47f-6c6b-4ce3-8cd7-d83136d68f96",
   "metadata": {},
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e665a3-7639-4d43-bb76-ef894bd2f8d1",
   "metadata": {},
   "source": [
    "We have already studied in logistic regression we squash the best fit line with the help of sigmoid activation function. The sigmoid activation function gives output for a provided input equation between a range of 0 and 1. That is how it helps in squashing the best fit line. Sigmoid function is denoted by σ ans is given as \n",
    "σ=1/(1+e^(-z) )\n",
    "Here,\n",
    "\tz=h_θ (x)=θ_0+θ_1 x_1\n",
    "This equation can also be written as: h_θ (x)=σ〖(θ〗_0+θ_1 x_1)\n",
    "〖h〗_θ (x)=1/(1+e^(-z) )\n",
    "h_θ (x)=1/(1+e^(-〖(θ〗_0+θ_1 x_1)) )\n",
    "This is the final equation for logistic regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23716025-57b7-4f56-b8b2-7e1dbbba9e5c",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb401a8-5806-4497-9573-eb1020325230",
   "metadata": {},
   "source": [
    "In ridge regression we have seen that we use L2 regularization and in lasso regression we use L1 regularization and in elastic net regression we combine both L1 and L2 regularization. Same regularization parameters are also used in logistic regression.\n",
    "L2 REGULARIZATION\n",
    "Used for reducing overfitting and is given by:\n",
    "J(θ_0,θ_1 )=-ylog(h_θ (x))-(1-y)log(1-h_θ (x))+λ∑_(i=1)^n▒(Slope)^2 \n",
    "L1 REGULARIZATION\n",
    "It is used for feature selection and is given by:\n",
    "J(θ_0,θ_1 )=-ylog(h_θ (x))-(1-y)log(1-h_θ (x))+λ∑_(i=1)^n▒|Slope| \n",
    "ELASTIC NET \n",
    "It is combination of L1 and L2 regularization and is given by:\n",
    "J(θ_0,θ_1 )=-ylog(h_θ (x))-(1-y)log(1-h_θ (x))+λ_1 ∑_(i=1)^n▒(Slope)^2 +λ_2 ∑_(i=1)^n▒|Slope| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b829bb-ba15-43de-bc5a-c3e3173559b0",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbe486-a685-4c90-8f1f-25afda93af38",
   "metadata": {},
   "source": [
    "In ridge regression we have seen that we use L2 regularization and in lasso regression we use L1 regularization and in elastic net regression we combine both L1 and L2 regularization. Same regularization parameters are also used in logistic regression.\n",
    "L2 REGULARIZATION\n",
    "Used for reducing overfitting and is given by:\n",
    "J(θ_0,θ_1 )=-ylog(h_θ (x))-(1-y)log(1-h_θ (x))+λ∑_(i=1)^n▒(Slope)^2 \n",
    "L1 REGULARIZATION\n",
    "It is used for feature selection and is given by:\n",
    "J(θ_0,θ_1 )=-ylog(h_θ (x))-(1-y)log(1-h_θ (x))+λ∑_(i=1)^n▒|Slope| \n",
    "ELASTIC NET \n",
    "It is combination of L1 and L2 regularization and is given by:\n",
    "J(θ_0,θ_1 )=-ylog(h_θ (x))-(1-y)log(1-h_θ (x))+λ_1 ∑_(i=1)^n▒(Slope)^2 +λ_2 ∑_(i=1)^n▒|Slope| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950a039-afb3-448b-890d-a1103cb2e2d2",
   "metadata": {},
   "source": [
    "\n",
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee80de-cbf6-45ee-9d7e-20343d083bc9",
   "metadata": {},
   "source": [
    "We cannot solve classification problem with linear regression because:\n",
    "\tIf we draw a best fit line, the presence of outlier can completely change the course of best fit line which may give inaccurate predictions.\n",
    "\tAlso output may cross 1 or go below 0 but since it is a binary problem the output must be between 1 and 0.\n",
    "\tIn order to overcome above condition we need to use a technique called squash. In this technique, we squash the best fit line which is not possible in linear regression\n",
    "To overcome this we use logistic regression. In order to squash the best fit line we use sigmoid function.\n",
    "In the graph shown here, on the x-axis is the number of play hours and on the y-axis is the result i.e. pass = 1 or fail = 0. Yellow points marked at point 1 are passes and red points marked at 0 are fails. A best fit line is drawn as shown. The equation h_θ (x)=θ_0+θ_1 x_1 used in linear regression will be used here too for finding the best fit line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d7d08-5f70-4350-820b-1bf4cb35bc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
