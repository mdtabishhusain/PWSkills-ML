{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dcac7-a831-4102-86bb-e24311579892",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcedd401-db9f-49c4-a175-f4e67109a440",
   "metadata": {},
   "source": [
    "In R Square instead of finding the best fit line we found average output and this average output becomes best fit line. Then this will be all the predicted average points and we will try to find out the difference between actual and predicted average points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49a79f-ffdb-4bf8-9eb0-59ce682e04eb",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6adcfbf-6fda-4cee-86bc-26c7d4b284ef",
   "metadata": {},
   "source": [
    "As we have studied linear regression in this chapter, we know we may have a linear regression equation with one independent feature or may also have multiple independent feature. As we go on adding new and new independent features our R^2 value usually increases.\n",
    "It happens always if we go on adding independent features but we should understand that this R^2 value, as we go on adding independent features, will never decrease. Even though we are adding some independent features what may happen is that the particular feature may not be co related with the target output. But even though we are adding it the R^2 value is increasing. R^2 is not penalising the new added features. \n",
    "To overcome this problem we use Adjusted R^2. Adjusted R^2 is given by \n",
    "R_adjusted^2=1-((1-R^2 )  (N-1))/(N-p-1)\n",
    "Where,\n",
    "R^2 is sample R – square\n",
    "p is number of predictors (independent features) \n",
    "N is total sample size of dataset\n",
    "It penalizes attributes that are not correlated to the target output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e680b-1232-4aa7-9bc3-2a612d69a0c4",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba84001c-686c-48c0-b138-3b64c09845d1",
   "metadata": {},
   "source": [
    "It penalizes attributes that are not correlated to the target output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957389ac-8c5c-4dcf-a7e0-3fc26bbabe80",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08bf2a-ec70-4bb2-9452-3d40042a901d",
   "metadata": {},
   "source": [
    "\tMean Squared Error (MSE): Linear regression considers mean squared error as default cost function. It is denoted by:\n",
    "MSE=1/n ∑_(i=1)^n▒(y_i-y ̂_i )^2 \n",
    "Where,\n",
    "\tn is the number of data points\n",
    "\ty_(i )are the actual value\n",
    "\t(y_i ) ̂  are the predicted points\n",
    "Advantages:\n",
    "\tThis equation is differentiable.\n",
    "\tIt has only one local or global minima.\n",
    "Disadvantages\n",
    "\tIt isn’t robust to outliers.\n",
    "\tIt isn’t in the same unit.\n",
    "\tMean Absolute Error (MAE)\n",
    "MAE=1/n ∑_(i=1)^n▒|y-y ̂ | \n",
    "Advantage\n",
    "\tRobust to outliers.\n",
    "\tIt will be in the same unit\n",
    "Disadvantage\n",
    "\tConvergence usually takes more time\n",
    "\tRoot Mean Square Error (RMSE)\n",
    "√(1/n ∑_(i=1)^n▒(y_i-y ̂_i )^2 )\n",
    "Advantage\n",
    "\tIt is differentiable.\n",
    "\tIt will be in the same unit\n",
    "Disadvantage\n",
    "Not robust to outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe6d90-d514-4333-89af-6a84865eef63",
   "metadata": {},
   "source": [
    "\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90027480-e131-4437-9d38-693c0ad500c2",
   "metadata": {},
   "source": [
    "\tMean Squared Error (MSE): Linear regression considers mean squared error as default cost function. It is denoted by:\n",
    "MSE=1/n ∑_(i=1)^n▒(y_i-y ̂_i )^2 \n",
    "Where,\n",
    "\tn is the number of data points\n",
    "\ty_(i )are the actual value\n",
    "\t(y_i ) ̂  are the predicted points\n",
    "Advantages:\n",
    "\tThis equation is differentiable.\n",
    "\tIt has only one local or global minima.\n",
    "Disadvantages\n",
    "\tIt isn’t robust to outliers.\n",
    "\tIt isn’t in the same unit.\n",
    "\tMean Absolute Error (MAE)\n",
    "MAE=1/n ∑_(i=1)^n▒|y-y ̂ | \n",
    "Advantage\n",
    "\tRobust to outliers.\n",
    "\tIt will be in the same unit\n",
    "Disadvantage\n",
    "\tConvergence usually takes more time\n",
    "\tRoot Mean Square Error (RMSE)\n",
    "√(1/n ∑_(i=1)^n▒(y_i-y ̂_i )^2 )\n",
    "Advantage\n",
    "\tIt is differentiable.\n",
    "\tIt will be in the same unit\n",
    "Disadvantage\n",
    "Not robust to outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e1a29-6018-4076-93b6-9c69c082f15c",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d6569-5f47-4bd0-9eac-e9e6d8a958d5",
   "metadata": {},
   "source": [
    "Lasso regression is a L1 regularization machine learning algorithm used in feature selection. Feature selection is the process of selecting the most important features of a dataset. Lasso regression uses absolute value of slope in its cost function to achieve the target of selecting most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf38b525-d78f-439c-93d3-49e5b34f5d63",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863fc8cc-2a4a-4d46-bd6c-8684a69b6c6a",
   "metadata": {},
   "source": [
    "Ridge regression is a L2 regularization machine learning algorithm used for reducing overfitting. We know overfitting occurs when training dataset has high accuracy but the test dataset has low accuracy. Ridge regression states that to overcome this situation, instead to reducing error to 0, keep some errors and draw the best fit line in a bit unconventional method than linear regression (as shown in graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feac609c-63b9-4e8d-aee8-d254f7d64c68",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ba61b-a80a-4624-90c0-2c7fdad31c53",
   "metadata": {},
   "source": [
    "ELASTIC NET REGRESSION\n",
    "Elastic net regression is the combination of ridge and lasso regression. That means it can reduce overfitting and at the same time can also do feature selection.\n",
    "Its cost function is given as \n",
    "Cost fn=1/n ∑_(i=1)^n▒(y_i-(y_i ) ̂ )^2 +λ_1 ∑_(i=1)^n▒(Slope)^2 +λ_2 ∑_(i=1)^n▒|Slope| \n",
    "λ_1  & λ_2 are selected by hyper parameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305971bd-c002-4390-a8ff-16755992ecaf",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fe376-6b4a-42e6-af3f-8240d3dec7ed",
   "metadata": {},
   "source": [
    "Model A \n",
    "Elastic net regression is the combination of ridge and lasso regression. That means it can reduce overfitting and at the same time can also do feature selection.\n",
    "Its cost function is given as \n",
    "Cost fn=1/n ∑_(i=1)^n▒(y_i-(y_i ) ̂ )^2 +λ_1 ∑_(i=1)^n▒(Slope)^2 +λ_2 ∑_(i=1)^n▒|Slope| \n",
    "λ_1  & λ_2 are selected by hyper parameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc1101-08d1-4499-8ec6-39fcf3696db2",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5831f63-192d-4713-a995-b54218933bf3",
   "metadata": {},
   "source": [
    "Model B\n",
    "Elastic net regression is the combination of ridge and lasso regression. That means it can reduce overfitting and at the same time can also do feature selection.\n",
    "Its cost function is given as \n",
    "Cost fn=1/n ∑_(i=1)^n▒(y_i-(y_i ) ̂ )^2 +λ_1 ∑_(i=1)^n▒(Slope)^2 +λ_2 ∑_(i=1)^n▒|Slope| \n",
    "λ_1  & λ_2 are selected by hyper parameter tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec368c33-5282-464f-8bd3-76432523d985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
