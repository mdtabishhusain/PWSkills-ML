{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e2be7b-5755-4e2d-99d2-1688359e1d72",
   "metadata": {},
   "source": [
    "Q1. What is hierarchical clustering, and how is it different from other clustering techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d88344-15fb-4676-bc7b-df65b76fabcf",
   "metadata": {},
   "source": [
    "Hierarchal clustering is a type of unsupervised machine learning algorithm and it is also a type of clustering algorithm. As we already know K Means clustering algorithm use centroids to make clusters of data points. But hierarchal clustering helps in making clusters of data without using any centroid. This is the major difference between K Means and Hierarchal clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b7a193-d898-4040-855d-573dc2bb6b90",
   "metadata": {},
   "source": [
    "Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df576cc8-a38c-48da-88dd-eb8cf986f7b9",
   "metadata": {},
   "source": [
    "Hierarchal clustering is of 2 types:\n",
    "i.\tAgglomerative\n",
    "ii.\tDivisive\n",
    "Both are alternate to each other we will study about agglomerative and then its opposite will be divisive. In agglomerative we step by step make clusters of data points. In divisive method we do just opposite of agglomerative. Here we break the clusters into data points like first we break the single large cluster. Then we break the cluster inside it. Then we keep repeating until we get separate data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa23207-f3d1-41db-a475-5213cd66a503",
   "metadata": {},
   "source": [
    "Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the\n",
    "common distance metrics used?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0980fc3-c727-4aa5-bf8a-5e27292dda9a",
   "metadata": {},
   "source": [
    "The distance between data points is calculated using Euclidian distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e36f0f-ecce-41a0-91b8-4f9d7d16ee1c",
   "metadata": {},
   "source": [
    "Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some\n",
    "common methods used for this purpose?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300151a8-8f9c-41c6-b94e-ad113dce3fae",
   "metadata": {},
   "source": [
    "Determining Optimal Number of Clusters in Hierarchical Clustering:\n",
    "\n",
    "Dendrogram Analysis:\n",
    "\n",
    "Method: Examine the dendrogram (tree-like structure) to identify significant vertical distances, suggesting the number of clusters.\n",
    "Agglomerative Coefficient (Cophenetic Correlation):\n",
    "\n",
    "Method: Measure the correlation between the distances in the dendrogram and the original pairwise distances.\n",
    "Gap Statistics:\n",
    "\n",
    "Method: Compare the within-cluster dispersion of the hierarchical clustering with that of a random dataset to identify an optimal number of clusters.\n",
    "Silhouette Score:\n",
    "\n",
    "Method: Evaluate the average silhouette score for different cluster numbers, choosing the number that maximizes cohesion within clusters and separation between clusters.\n",
    "Calinski-Harabasz Index:\n",
    "\n",
    "Method: Calculate the ratio of the between-cluster variance to within-cluster variance, selecting the number of clusters that maximizes this ratio.\n",
    "Elbow Method (for K-means on Flattened Data):\n",
    "\n",
    "Method: Apply hierarchical clustering to obtain a flat clustering, then use the elbow method on the resulting clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30272a81-971f-4253-a329-42dfda90ed4b",
   "metadata": {},
   "source": [
    "Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d49e2-6442-464a-a598-8bf431e43416",
   "metadata": {},
   "source": [
    "The number of clusters to be formed is selected by using a dendrogram of data point and the Euclidian distance. The number cluster is decided by a threshold value. The threshold value is selected by passing a line through the longest vertical line through which no horizontal line passes. The number of cross section made by threshold line is the number of clusters that should be formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0a28e-7ad8-4e88-9d99-0b0292986a5a",
   "metadata": {},
   "source": [
    "Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the\n",
    "distance metrics different for each type of data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6305c5-cf6f-4faf-9890-a90a9f25e033",
   "metadata": {},
   "source": [
    "Yes, hierarchical clustering can be used for both numerical and categorical data.\n",
    "\n",
    "For Numerical Data:\n",
    "\n",
    "Distance Metric: Common distance metrics include Euclidean distance, Manhattan distance, or correlation-based distances.\n",
    "For Categorical Data:\n",
    "\n",
    "Distance Metric: Use metrics suitable for categorical data, such as the Jaccard coefficient, which measures the proportion of shared non-zero elements between two sets.\n",
    "Mixed Data Types:\n",
    "\n",
    "Distance Metric: For datasets with a mix of numerical and categorical variables, distance metrics like Gower's distance or the Generalized Mahalanobis distance can be used to handle both types of data simultaneously.\n",
    "Selecting appropriate distance metrics is crucial for capturing the similarity or dissimilarity between data points based on their respective types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831114e8-6b39-46d5-b763-557b2d3365f8",
   "metadata": {},
   "source": [
    "Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab0d56-e1bd-4bef-93f1-ae7901d3c52a",
   "metadata": {},
   "source": [
    "Using Hierarchical Clustering to Identify Outliers or Anomalies:\n",
    "\n",
    "Dendrogram Inspection:\n",
    "\n",
    "Process: Examine the dendrogram to identify branches with a small number of data points.\n",
    "Insight: Outliers may appear as isolated branches or singletons.\n",
    "Cutting the Dendrogram:\n",
    "\n",
    "Process: Cut the dendrogram at a certain height to form clusters.\n",
    "Insight: Isolated or small clusters may indicate potential outliers.\n",
    "Observing Cluster Sizes:\n",
    "\n",
    "Process: Check for clusters significantly smaller than others.\n",
    "Insight: Smaller clusters may contain outliers or less typical observations.\n",
    "Distance Metrics:\n",
    "\n",
    "Process: Utilize distance metrics that emphasize dissimilarity for outlier detection.\n",
    "Insight: Outliers will have larger dissimilarities with other data points.\n",
    "Silhouette Analysis:\n",
    "\n",
    "Process: Calculate silhouette scores for each point in the dataset.\n",
    "Insight: Lower silhouette scores indicate points that may be outliers.\n",
    "By leveraging hierarchical clustering and examining the resulting structures, you can identify data points that deviate significantly from the overall patterns in the dataset, potentially indicating outliers or anomalies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
