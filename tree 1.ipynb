{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b10412d9-e339-4f8a-8ea6-bb1239f5b18c",
   "metadata": {},
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91051a9-fb1c-443c-93c1-783678cd82c0",
   "metadata": {},
   "source": [
    "Decision Tree is a supervised learning technique that can be used for both classification and Regression problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome. There are two nodes, which are the Decision Node and Leaf Node. Decision nodes are used to make any decision and have multiple branches, whereas Leaf nodes are the output of those decisions and do not contain any further branches.\n",
    "It is called a decision tree because, similar to a tree, it starts with the root node, which expands on further branches and constructs a tree-like structure.\n",
    "In order to build a tree, there are 2 algorithms that can be used:\n",
    "i.\tID3 (Iterative Dichotomiser 3): In ID3, the root node can be split into multiple decision nodes (more than 2). Also all further splits can be multiple.\n",
    "ii.\tCART (Classification and Regression Tree): In CART, there is binary split in the root node there for only 2 decision nodes form. Also all further splits are binary.Generally in python the sci-kit library uses CART to form decision trees. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3cd5c-e4ae-4d49-b5df-f78166d1cb2d",
   "metadata": {},
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eda930-8758-4917-a7a0-9a4d07464658",
   "metadata": {},
   "source": [
    "Let's understand how a decision tree is made using a simple example. In python we perform loop functions. Let’s suppose we have a variable age = 14\n",
    "if (age ≤ 15): \n",
    "\tprint(“The person is in school”) \n",
    "elif (age > 15 and age ≤ 21): \n",
    "\tprint(“The person is in college”) \n",
    "else: \n",
    "\tprint(“The person has passed college”) \n",
    "outlook\ttemp\thumidity\twind\tplay\n",
    "Sunny\tHot\tHigh\tWeak\tNo\n",
    "Sunny\tHot\tHigh\tStrong\tNo\n",
    "Overcast\tHot\tHigh\tWeak\tYes\n",
    "Rain\tMild\tHigh\tWeak\tYes\n",
    "Rain\tCool\tNormal\tWeak\tYes\n",
    "Rain\tCool\tNormal\tStrong\tNo\n",
    "Overcast\tCool\tNormal\tStrong\tYes\n",
    "Sunny\tMild\tHigh\tWeak\tNo\n",
    "Sunny\tCool\tNormal\tWeak\tYes\n",
    "Rain\tMild\tNormal\tWeak\tYes\n",
    "Sunny\tMild\tNormal\tStrong\tYes\n",
    "Overcast\tMild\tHigh\tStrong\tYes\n",
    "Overcast\tHot\tNormal\tWeak\tYes\n",
    "Rain\tMild\tHigh\tStrong\tNo\n",
    "We can see there are three conditions in the above loop. If we construct its structure in form of a binary tree or a tree then first node will be ≤15 and this node will have 2 conditions – Yes or No. If yes then we will get a leaf node i.e. “The person is in school”. If no we will get a node which will be >15 and ≤21. Again this node will have 2 conditions – Yes or No. If yes then we will get node “The person is in college”. If no then we will get node “The person has passed college”.\n",
    "We can see we have created a decision tree using this simple for loop function. We will do the same thing for dataset. We will study how we will create a decision tree for different variables or features of the dataset.\n",
    "Here is the play tennis dataset. The features such as ‘outlook’, ‘temp’, ‘humidity’ and ‘wind’ are independent features. Whereas the feature ‘play’ is a dependent feature. Based on these features we have to predict whether the person will play tennis or not. In this type of scenarios we will see how we have to split the dataset. To construct a decision tree we will take a feature ‘outlook’ from play tennis dataset. \n",
    "There are 3 unique categories in feature 'outlook' i.e. sunny, overcast and rain. We will split the feature 'outlook' according to these 3 sub categories. We can see there are 9 yes and 5 no in this feature. In sunny sub category, there are 2 yes and 3 no. In overcast sub category, there are 4 yes and 0 no. In rain sub category, there are 3 yes and 2 no. \n",
    "We can see in the overcast sub category there are only yes condition available. These types of nodes where either affirmative or negative conditions are available are of pure split and nodes which have both affirmative and negative conditions like sunny and rain are considered as impure nodes and they have impure split. Our aim is to splitting the impure nodes unless or until we get it in form of leaf nodes with pure split.\n",
    "So here 2 important questions arises; how to check purity of nodes and how the features are selected. To check purity of a node we have 2 methods: Entropy and Gini Impurity or Index. And to determine feature selection we use Information Gain. We will study each of these in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c00fb-b8a2-4339-bcb5-49a96329b9f5",
   "metadata": {},
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582481d8-2db7-4350-b753-82393d71bae9",
   "metadata": {},
   "source": [
    "Let's understand how a decision tree is made using a simple example. In python we perform loop functions. Let’s suppose we have a variable age = 14\n",
    "if (age ≤ 15): \n",
    "\tprint(“The person is in school”) \n",
    "elif (age > 15 and age ≤ 21): \n",
    "\tprint(“The person is in college”) \n",
    "else: \n",
    "\tprint(“The person has passed college”) \n",
    "outlook\ttemp\thumidity\twind\tplay\n",
    "Sunny\tHot\tHigh\tWeak\tNo\n",
    "Sunny\tHot\tHigh\tStrong\tNo\n",
    "Overcast\tHot\tHigh\tWeak\tYes\n",
    "Rain\tMild\tHigh\tWeak\tYes\n",
    "Rain\tCool\tNormal\tWeak\tYes\n",
    "Rain\tCool\tNormal\tStrong\tNo\n",
    "Overcast\tCool\tNormal\tStrong\tYes\n",
    "Sunny\tMild\tHigh\tWeak\tNo\n",
    "Sunny\tCool\tNormal\tWeak\tYes\n",
    "Rain\tMild\tNormal\tWeak\tYes\n",
    "Sunny\tMild\tNormal\tStrong\tYes\n",
    "Overcast\tMild\tHigh\tStrong\tYes\n",
    "Overcast\tHot\tNormal\tWeak\tYes\n",
    "Rain\tMild\tHigh\tStrong\tNo\n",
    "We can see there are three conditions in the above loop. If we construct its structure in form of a binary tree or a tree then first node will be ≤15 and this node will have 2 conditions – Yes or No. If yes then we will get a leaf node i.e. “The person is in school”. If no we will get a node which will be >15 and ≤21. Again this node will have 2 conditions – Yes or No. If yes then we will get node “The person is in college”. If no then we will get node “The person has passed college”.\n",
    "We can see we have created a decision tree using this simple for loop function. We will do the same thing for dataset. We will study how we will create a decision tree for different variables or features of the dataset.\n",
    "Here is the play tennis dataset. The features such as ‘outlook’, ‘temp’, ‘humidity’ and ‘wind’ are independent features. Whereas the feature ‘play’ is a dependent feature. Based on these features we have to predict whether the person will play tennis or not. In this type of scenarios we will see how we have to split the dataset. To construct a decision tree we will take a feature ‘outlook’ from play tennis dataset. \n",
    "There are 3 unique categories in feature 'outlook' i.e. sunny, overcast and rain. We will split the feature 'outlook' according to these 3 sub categories. We can see there are 9 yes and 5 no in this feature. In sunny sub category, there are 2 yes and 3 no. In overcast sub category, there are 4 yes and 0 no. In rain sub category, there are 3 yes and 2 no. \n",
    "We can see in the overcast sub category there are only yes condition available. These types of nodes where either affirmative or negative conditions are available are of pure split and nodes which have both affirmative and negative conditions like sunny and rain are considered as impure nodes and they have impure split. Our aim is to splitting the impure nodes unless or until we get it in form of leaf nodes with pure split.\n",
    "So here 2 important questions arises; how to check purity of nodes and how the features are selected. To check purity of a node we have 2 methods: Entropy and Gini Impurity or Index. And to determine feature selection we use Information Gain. We will study each of these in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd42a1-d5c9-4331-af60-957e401cb63c",
   "metadata": {},
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f78b6b-83d9-4840-9bc1-151a6e3d3b28",
   "metadata": {},
   "source": [
    "Let's understand how a decision tree is made using a simple example. In python we perform loop functions. Let’s suppose we have a variable age = 14\n",
    "if (age ≤ 15): \n",
    "\tprint(“The person is in school”) \n",
    "elif (age > 15 and age ≤ 21): \n",
    "\tprint(“The person is in college”) \n",
    "else: \n",
    "\tprint(“The person has passed college”) \n",
    "outlook\ttemp\thumidity\twind\tplay\n",
    "Sunny\tHot\tHigh\tWeak\tNo\n",
    "Sunny\tHot\tHigh\tStrong\tNo\n",
    "Overcast\tHot\tHigh\tWeak\tYes\n",
    "Rain\tMild\tHigh\tWeak\tYes\n",
    "Rain\tCool\tNormal\tWeak\tYes\n",
    "Rain\tCool\tNormal\tStrong\tNo\n",
    "Overcast\tCool\tNormal\tStrong\tYes\n",
    "Sunny\tMild\tHigh\tWeak\tNo\n",
    "Sunny\tCool\tNormal\tWeak\tYes\n",
    "Rain\tMild\tNormal\tWeak\tYes\n",
    "Sunny\tMild\tNormal\tStrong\tYes\n",
    "Overcast\tMild\tHigh\tStrong\tYes\n",
    "Overcast\tHot\tNormal\tWeak\tYes\n",
    "Rain\tMild\tHigh\tStrong\tNo\n",
    "We can see there are three conditions in the above loop. If we construct its structure in form of a binary tree or a tree then first node will be ≤15 and this node will have 2 conditions – Yes or No. If yes then we will get a leaf node i.e. “The person is in school”. If no we will get a node which will be >15 and ≤21. Again this node will have 2 conditions – Yes or No. If yes then we will get node “The person is in college”. If no then we will get node “The person has passed college”.\n",
    "We can see we have created a decision tree using this simple for loop function. We will do the same thing for dataset. We will study how we will create a decision tree for different variables or features of the dataset.\n",
    "Here is the play tennis dataset. The features such as ‘outlook’, ‘temp’, ‘humidity’ and ‘wind’ are independent features. Whereas the feature ‘play’ is a dependent feature. Based on these features we have to predict whether the person will play tennis or not. In this type of scenarios we will see how we have to split the dataset. To construct a decision tree we will take a feature ‘outlook’ from play tennis dataset. \n",
    "There are 3 unique categories in feature 'outlook' i.e. sunny, overcast and rain. We will split the feature 'outlook' according to these 3 sub categories. We can see there are 9 yes and 5 no in this feature. In sunny sub category, there are 2 yes and 3 no. In overcast sub category, there are 4 yes and 0 no. In rain sub category, there are 3 yes and 2 no. \n",
    "We can see in the overcast sub category there are only yes condition available. These types of nodes where either affirmative or negative conditions are available are of pure split and nodes which have both affirmative and negative conditions like sunny and rain are considered as impure nodes and they have impure split. Our aim is to splitting the impure nodes unless or until we get it in form of leaf nodes with pure split.\n",
    "So here 2 important questions arises; how to check purity of nodes and how the features are selected. To check purity of a node we have 2 methods: Entropy and Gini Impurity or Index. And to determine feature selection we use Information Gain. We will study each of these in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d758dbe5-b844-464e-ba2d-9dca316c0a1c",
   "metadata": {},
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c326b-e6db-4557-a4bc-32c6bd83d85a",
   "metadata": {},
   "source": [
    "Confusion Matrix: In binary classification, confusion matrix is a 2 X 2 matrix where columns are the actual values of the model, i.e. y and rows are the predicted values of the model, i.e. y ̂. Confusion matrix is shown in the table.\n",
    "TP and TN are True positive and True negative respectively. True positive is when both actual and predicted values are and true negative is when both the values are 0. So TP and TN are the correct predictions. FP and FN are False positive and False negative respectively. False positive when actual value is 0 and predicted value is 1 and false negative is when actual value is 1 and predicted value is 0. So FP and FN are incorrect predictions\n",
    "\t1\t0\n",
    "1\tTP=3\tFP=2\n",
    "0\tFN=1\tTN=1\n",
    "The accuracy of the model is given by\n",
    "Model Acc=(TP+TN)/(TP+TN+FP+FN)\n",
    "Suppose in a model total True positives are 3 and False positive is 1 and total True negatives are 2 and False negative is 1 then the matrix formed will be: \n",
    "The accuracy of the model is then given by \n",
    "Model acc=(3+1)/(3+2+1+1)=4/7=0.57=57%\n",
    "The disadvantage with confusion matrix is that it cannot be used in an imbalanced dataset scenario. For imbalanced dataset we use precision and recall. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50371376-2df8-40e4-abe6-a1f19ef746c0",
   "metadata": {},
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d20e04-6b35-49b2-8150-ecd1df961352",
   "metadata": {},
   "source": [
    "TP and TN are True positive and True negative respectively. True positive is when both actual and predicted values are and true negative is when both the values are 0. So TP and TN are the correct predictions. FP and FN are False positive and False negative respectively. False positive when actual value is 0 and predicted value is 1 and false negative is when actual value is 1 and predicted value is 0. So FP and FN are incorrect predictions\n",
    "\t1\t0\n",
    "1\tTP=3\tFP=2\n",
    "0\tFN=1\tTN=1\n",
    "The accuracy of the model is given by\n",
    "Model Acc=(TP+TN)/(TP+TN+FP+FN)\n",
    "Suppose in a model total True positives are 3 and False positive is 1 and total True negatives are 2 and False negative is 1 then the matrix formed will be: \n",
    "The accuracy of the model is then given by \n",
    "Model acc=(3+1)/(3+2+1+1)=4/7=0.57=57%\n",
    "The disadvantage with confusion matrix is that it cannot be used in an imbalanced dataset scenario. For imbalanced dataset we use precision and recall. \n",
    "\tPrecision: Precision states that out of all the actual values, how many are correctly predicted. Here the false positive is reduced and the importance of false positive is also taken into consideration. Precision is given by:\n",
    "Precision=TP/(TP+FP)\n",
    "\tRecall: Recall states that out of all the predicted values, how many are correctly predicted with actual values. Here false negative is reduced and the importance of false negative is also taken into consideration. Recall is given by:\n",
    "Recall=TP/(TP+FN)\n",
    "\tF – Beta Score: F – Beta Score takes both, precision and recall into consideration as important. It is given by:\n",
    "F-β Score=(1+β^2 )  (Precision × Recall)/(Precision+Recall)\n",
    "There are 3 condition in F Beta Score:\n",
    "\tIf both FP and FN are important then\n",
    "β=1 and F 1 score=2×(Precision × Recall)/(Precision+Recall)\n",
    "This is known as harmonic mean\n",
    "\tIf FP is more important than FN then\n",
    "β=0.5 and F 0.5 score=1.25×(Precision × Recall)/(Precision+Recall)\n",
    "\tIf FN is more important than FN then \n",
    "β=2 and F 2 score=5×(Precision × Recall)/(Precision+Recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a116a6-3484-46a8-9f8a-0840b4c7a44e",
   "metadata": {},
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "explain how this can be done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e133689-9c46-4ef0-98aa-3d4bb4040e20",
   "metadata": {},
   "source": [
    "We have already learned that a dataset is divided into 3 parts train, validation and test. The train and validation data set is used for training the model while test dataset is used for testing the model. First the dataset is divided into train and test. Then the training dataset is divided into train and validation dataset. This separation is done with the help of train_test_split module of sklearn library. While splitting the dataset we give a value called test_size and random_state. The test_size determines the size of test dataset and random_state is used to determine size of validation dataset.\n",
    "But there is a catch. For different values of random_state we get different accuracy for the same training data. For example if random_state is 100 we might get accuracy of 85% or if random_state is 42 accuracy might be 70%. So to overcome this situation cross validation is used. \n",
    "Cross validation takes out the mean of all accuracy obtain by changing the random_state. For example if the random_state is changed thrice and the accuracy in these cases are 80%, 85% and 78% then the final accuracy after cross validation will be, (80+85+78)/3=81%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a80508-0cb0-4434-a89a-f7bcf51d9784",
   "metadata": {},
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19e1185-734e-4fd5-9729-a660f3c966bf",
   "metadata": {},
   "source": [
    "We have already learned that a dataset is divided into 3 parts train, validation and test. The train and validation data set is used for training the model while test dataset is used for testing the model. First the dataset is divided into train and test. Then the training dataset is divided into train and validation dataset. This separation is done with the help of train_test_split module of sklearn library. While splitting the dataset we give a value called test_size and random_state. The test_size determines the size of test dataset and random_state is used to determine size of validation dataset.\n",
    "But there is a catch. For different values of random_state we get different accuracy for the same training data. For example if random_state is 100 we might get accuracy of 85% or if random_state is 42 accuracy might be 70%. So to overcome this situation cross validation is used. \n",
    "Cross validation takes out the mean of all accuracy obtain by changing the random_state. For example if the random_state is changed thrice and the accuracy in these cases are 80%, 85% and 78% then the final accuracy after cross validation will be, (80+85+78)/3=81%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b006c4-e3e9-410b-b39e-bdaff102fb08",
   "metadata": {},
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea37c83-a673-4d8a-8f17-38ca16673555",
   "metadata": {},
   "source": [
    "We have already learned that a dataset is divided into 3 parts train, validation and test. The train and validation data set is used for training the model while test dataset is used for testing the model. First the dataset is divided into train and test. Then the training dataset is divided into train and validation dataset. This separation is done with the help of train_test_split module of sklearn library. While splitting the dataset we give a value called test_size and random_state. The test_size determines the size of test dataset and random_state is used to determine size of validation dataset.\n",
    "But there is a catch. For different values of random_state we get different accuracy for the same training data. For example if random_state is 100 we might get accuracy of 85% or if random_state is 42 accuracy might be 70%. So to overcome this situation cross validation is used. \n",
    "Cross validation takes out the mean of all accuracy obtain by changing the random_state. For example if the random_state is changed thrice and the accuracy in these cases are 80%, 85% and 78% then the final accuracy after cross validation will be, (80+85+78)/3=81%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3823e-dc7a-462e-a17e-1a5ab826dda7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
