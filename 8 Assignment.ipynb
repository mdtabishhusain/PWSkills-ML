{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06e2d017-745e-44bf-936a-a37b8f46264f",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5282e4cc-4471-47bb-8136-9366241f641d",
   "metadata": {},
   "source": [
    "Gradient boosting is another popular boosting algorithm in machine learning used for classification and regression tasks. It follows the method of boosting technique in each models and they are combined sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91c45b-9911-4667-aacd-a4762cc37d3e",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7f8684c-924d-4f3e-97fc-695816276a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0144\n",
      "R-squared: 0.9628\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate a sample dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.rand(100)\n",
    "\n",
    "# Define the number of boosting rounds and the learning rate\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Initialize the predictions with the mean of the target values\n",
    "initial_prediction = np.mean(y)\n",
    "predictions = np.full(len(y), initial_prediction)\n",
    "\n",
    "# Gradient boosting\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "for _ in range(n_estimators):\n",
    "    # Calculate residuals\n",
    "    residuals = y - predictions\n",
    "\n",
    "    # Fit a decision tree to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=3)\n",
    "    tree.fit(X, residuals)\n",
    "\n",
    "    # Make predictions with the current tree\n",
    "    tree_predictions = tree.predict(X)\n",
    "\n",
    "    # Update the predictions with a fraction (learning rate) of the new predictions\n",
    "    predictions += learning_rate * tree_predictions\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = np.mean((y - predictions) ** 2)\n",
    "\n",
    "# Calculate R-squared\n",
    "ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "ss_residual = np.sum((y - predictions) ** 2)\n",
    "r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r_squared:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3934736-f243-422a-8e5e-965885494ffa",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d815c676-199e-48d9-bce3-bf70339a910b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50}\n",
      "Mean Squared Error: 1.4696\n",
      "R-squared: 0.9989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=gb_regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a GradientBoostingRegressor with the best hyperparameters\n",
    "best_gb_regressor = GradientBoostingRegressor(**best_params, random_state=42)\n",
    "\n",
    "# Train the model on the full training set\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb95fc8-0074-4a89-86d8-83f96f07ebed",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6c999-3858-4831-a5d1-3d508a925e89",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a \"weak learner\" refers to a base model or a simple machine learning algorithm that performs slightly better than random guessing on a classification or regression problem. Weak learners are typically less complex models and are characterized by their limited capacity to capture the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d987a6-8a9f-4f9d-a2a2-d99ba35368f1",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7981d-03f2-4808-aa66-e0e63913f479",
   "metadata": {},
   "source": [
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to build a strong predictive model by sequentially combining a series of weak models, each of which focuses on correcting the errors made by the previous models. Gradient Boosting is a machine learning technique that emphasizes a \"boosting\" approach, where each weak learner is trained to address the shortcomings of the ensemble up to that point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b39010-c172-4fa3-9fe6-16a7d60a44e3",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2937e40b-7ee3-4e36-b1a0-4ca0152810b2",
   "metadata": {},
   "source": [
    "The key idea behind Gradient Boosting is that the ensemble focuses on the examples that are challenging to classify or predict correctly. Each weak learner is specialized in handling specific errors or patterns in the data. This leads to a strong model that can effectively capture complex relationships and provide accurate predictions. The contributions of weak learners are combined using weighted voting, with better-performing models having more influence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168b91d-e728-4a30-9c56-6dce8a5917d1",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b1ad4-86b6-4a3d-8d63-482a7fa9547b",
   "metadata": {},
   "source": [
    "We will be using gradient boosting to predict the output by following the following steps:\n",
    "\tFirst we will create a base model which will be a dumb model. It will take a specific input and will give its output. The output of base model will be average of the given output feature y and it is denoted by y ̂.\n",
    "\tAfter that we will calculate the residuals or errors by subtracting output of base model from each data point of output feature.\n",
    "Exp (x_1)\tDegree (x_2)\tSalary(y)\tR_1   (y-y ̂)\tR_2\n",
    "2\tB.E\t50K\t-25K\t-23K\n",
    "3\tMasters\t70K\t-5K\t-3K\n",
    "5\tMasters\t80K\t5K\t3K\n",
    "6\tPhD\t100K\t25K\t20K\n",
    "\t\ty ̂=75K\t\t\n",
    "\tNow we will construct a decision tree considering the x_1  & x_2 as input features and R_1 as output feature. This will give  a residual output R_2. After training the decision tree model with our input features (x_1& x_2) and output feature R_1, the predictions the decision tree will make will be near to the previous residual output as similar input features are provided to both R_1  & R_2. The outputs of R_2 shown in the table are assumed.\n",
    "Exp (x_1)\tDegree (x_2)\tSalary(y)\tR_1   (y-y ̂)\tR_2\ty ̂\n",
    "2\tB.E\t50K\t-25K\t-23K\t74.77\n",
    "3\tMasters\t70K\t-5K\t-3K\t74.97\n",
    "5\tMasters\t80K\t5K\t3K\t75.03\n",
    "6\tPhD\t100K\t25K\t20K\t75.2\n",
    "\t\ty ̂=75K\t\t\t\n",
    "\tNow we will calculate final prediction by adding the base model and predictions made by all the decision tree models (there is only one decision tree here). So here the output of base model for first record is 75 and output of first decision tree for the same record is -23. On summing both we get output of 52K which is near to the original output. We can see we get almost accurate prediction by using just a single decision tree but practically this is not an ideal situation. Rather it is overfitting of the model and during testing there is high possibility we get errors when new test data is passed. \n",
    "So to avoid this we use α which is the learning rate and is generally taken as 0.01 but it can range between 0 and 1. This α is multiplied with the decision trees.\n",
    "Exp (x_1)\tDegree (x_2)\tSalary(y)\tR_1   (y-y ̂)\tR_2\ty ̂\tR_3\n",
    "(y-y ̂)\n",
    "2\tB.E\t50K\t-25K\t-23K\t74.77\t-24.77\n",
    "3\tMasters\t70K\t-5K\t-3K\t74.97\t-4.97\n",
    "5\tMasters\t80K\t5K\t3K\t75.03\t4.97\n",
    "6\tPhD\t100K\t25K\t20K\t75.2\t24.8\n",
    "\t\ty ̂=75K\t\t\t\t\n",
    "Now we will add the output of base model for first record and output of first decision tree for the same record multiplied with the learning rate. This will give the output y ̂=74.77. We will do this for all records so that we get output of each record as shown in table.\n",
    "\tNow again we will calculate the residuals or errors by subtracting the obtained output feature y ̂ from each data point of output feature y for each of the input record. This residual will be considered as R_3  as shown in the table below.  \n",
    "\tNow we will construct another decision tree using this residual output R_3.\n",
    "\tThis whole sequential process will be repeated again and again and the mathematical function will be:\n",
    "F(x)=〖α_0 h〗_0 (x)+〖α_1 h〗_1 (x)+〖α_2 h〗_2 (x)+ ……+〖α_n h〗_n (x)\n",
    "It can be simplified as:\n",
    "F(x)=∑_(i=0)^n▒〖〖α_i h〗_i (x) 〗\n",
    "Where,\n",
    "α_i is the learning rate\n",
    "h_0 (x) is the output of base learner\n",
    "h_i (x) is the output of decision trees\n",
    "Also note that α_0 is considered as 1 and rest of the values of α_i ranges between 0 and 1. By default in python the value of α is 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20fccc-55f5-4c61-82cb-5652e77de8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
