{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6414ea44-07d5-4b3d-932f-4f12055dc0a1",
   "metadata": {},
   "source": [
    "Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84d7ecd-53e7-43d6-901a-5cd2c0fa21d3",
   "metadata": {},
   "source": [
    "Bagging aims to improve the accuracy and performance of machine learning algorithms. It does this by taking random subsets of an original dataset, with replacement, and fits either a classifier (for classification) or regressor (for regression) to each subset. The predictions for each subset are then aggregated through majority vote for classification or averaging for regression, increasing prediction accuracy.\n",
    "Let us understand this with help of an example. We have a training dataset with 1000 records. To make predictions we have taken random subsets of data from the training dataset and then used different ML models to train these subsets of data. These ML models are based on type of data (i.e. classifier for classification data and regressor for regression data). Note that the subsets will obviously be smaller than the training dataset. These models are called base learners and they are trained parallelly. The output of each model then aggregated either through voting for classification (the category with highest vote becomes prediction for new data) or through averaging for regression (the average value will be the prediction for new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a030c121-3684-4724-b325-c54f5f5c6c2c",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4a4ce-7d64-488b-ac0a-281247471379",
   "metadata": {},
   "source": [
    "A single model, also known as a base or weak learner, may not perform well individually due to high variance or high bias. However, when weak learners are aggregated, they can form a strong learner, as their combination reduces bias or variance, yielding better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a94057-cd06-4eb3-a5f7-bfe98a58e8ff",
   "metadata": {},
   "source": [
    "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6841e5d-71e1-465f-b940-94e8c87c6e86",
   "metadata": {},
   "source": [
    "Let us understand this with help of an example. We have a training dataset with 1000 records. To make predictions we have taken random subsets of data from the training dataset and then used different ML models to train these subsets of data. These ML models are based on type of data (i.e. classifier for classification data and regressor for regression data). Note that the subsets will obviously be smaller than the training dataset. These models are called base learners and they are trained parallelly. The output of each model then aggregated either through voting for classification (the category with highest vote becomes prediction for new data) or through averaging for regression (the average value will be the prediction for new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd968b-b115-4e2b-a1e6-ab0e0f52dd88",
   "metadata": {},
   "source": [
    "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75393d2-8a33-48d9-a7ac-e853508747d3",
   "metadata": {},
   "source": [
    "Random Forest is a successful method based on Bagging and Decision Trees. As we have already studied in bagging the subsets of the dataset are trained using different ML algorithms. But in random forest, only decision tree is used for training the subsets of the dataset. The subsets are formed by using row sampling and feature sampling. \n",
    "The row sampling is the subset of rows taken from the dataset. This subset of rows is less than total rows of the dataset (i.e. some of the rows from dataset are taken as sample).\n",
    "The feature sampling is the subset of features taken from the dataset. This subset of features is less than total features of the dataset (i.e. some of the features from dataset are taken as sample).\n",
    "Each subset might contain some common and some unique row sampling and feature sampling. The output of each model is aggregated based on the type of dataset. If dataset is of classification type then majority voting classifier is used. The output category with majority will be the output prediction for new dataset. If dataset is of regression type then average of the output of models will be the output prediction for new dataset.\n",
    "We know decision tree is prone to overfitting which gives a low bias training data and a high variance test data. To overcome this we use parameters like pre pruning and post pruning. But in Random Forest, since there are multiple decision trees, the training data is of low bias but the test data is also of low variance which is an ideal condition. Therefore Random Forest is preferred over Decision Tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a74251-0432-454d-9f0b-528dd602b5a0",
   "metadata": {},
   "source": [
    "\n",
    "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567beba3-4b03-4b9e-8480-47739d45b691",
   "metadata": {},
   "source": [
    "We have studied that during formation of subsets, rows and features are selected randomly. Some of them are common in other subsets while some are unique. But since they are randomly selected, there is a high possibility that some of the data is left behind in the dataset and it doesnâ€™t form any subset. This data is known as out of bag data. This is a very poor situation for training the model. \n",
    "To overcome this situation, the out of bag data is treated as validation data and using this validation data a validation score is provided. This validation score is known as out of bag score or OOB score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8312a-d4fb-4455-8f8b-a63b537efe14",
   "metadata": {},
   "source": [
    "Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaa4874-8fb8-4905-b473-759c034fcde5",
   "metadata": {},
   "source": [
    "Random Forests and bagging, in general, have found applications in various real-world scenarios:\n",
    "\n",
    "Medical Diagnosis: Random Forests can be used to predict medical conditions or disease outcomes based on patient data, such as symptoms, medical history, and test results. They are particularly useful when dealing with noisy and imbalanced datasets.\n",
    "\n",
    "Credit Scoring: Banks and financial institutions use Random Forests to assess the creditworthiness of loan applicants. By analyzing various financial and personal factors, they can make more accurate predictions of the likelihood of loan default.\n",
    "\n",
    "Image Classification: Bagging techniques, including Random Forests, are used in image classification tasks, such as identifying objects or faces in images. Each tree in the forest can focus on different aspects of the image.\n",
    "\n",
    "Anomaly Detection: In cybersecurity, Random Forests can be employed to detect network intrusions and anomalies in network traffic by learning patterns of normal and malicious behavior.\n",
    "\n",
    "Environmental Modeling: Random Forests can be used to model and predict various environmental factors, such as weather conditions, air quality, or ecological changes, by incorporating diverse sources of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bfdbe8-da65-4264-91f4-f3317523dc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
