{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd2f312-f077-4dfe-87bf-e59116818274",
   "metadata": {},
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values n some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values\n",
    "Design a pipeline that includes the following steps:\n",
    "- Use an automated feature selection method to identify the important features in the dataset\n",
    "- Create a numerical pipeline that includes the following steps: \n",
    "- Impute the missing values in the numerical columns using the mean of the column values\n",
    "- Scale the numerical columns using standardization\n",
    "- Create a categorical pipeline that includes the following steps\n",
    "- Impute the missing values in the categorical columns using the most frequent value of the column\n",
    "- One-hot encode the categorical columns\n",
    "- Combine the numerical and categorical pipelines using a Column Transformer\n",
    "- Use a Random Forest Classifier to build the final model\n",
    "- Evaluate the accuracy of the model on the test dataset\n",
    "\n",
    "Note! Your solution should include code snippets for each step of the pipeline, and a brief explanation of\n",
    "each step. You should also provide an interpretation of the results and suggest possible improvements for\n",
    "the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "804b5165-651d-4d74-af94-617e3c7ee2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:808: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create synthetic data as placeholders\n",
    "data = {\n",
    "    'NumericalFeature1': [1, 2, 3, 4, 5],\n",
    "    'NumericalFeature2': [0.5, 1.5, 2.5, 3.5, 4.5],\n",
    "    'CategoricalFeature1': ['A', 'B', 'A', 'B', 'C'],\n",
    "    'CategoricalFeature2': ['X', 'Y', 'X', 'Y', 'Z'],\n",
    "    'Target': [0, 1, 0, 1, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature selection using Random Forest (only on numerical features)\n",
    "numerical_features = ['NumericalFeature1', 'NumericalFeature2']\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_features = ['CategoricalFeature1', 'CategoricalFeature2']\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "# Combine numerical and one-hot encoded categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Final pipeline with feature selection and Random Forest Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78277d8-d21f-4215-b4cf-f60a39b3c532",
   "metadata": {},
   "source": [
    "Explanation of the pipeline steps:\n",
    "\n",
    "Data Splitting: The dataset is split into training and test sets, which is a standard practice in machine learning.\n",
    "\n",
    "Feature Selection: Random Forest is used as a feature selector to identify important features in the dataset. Features selected by the model are used in the subsequent steps.\n",
    "\n",
    "Numerical Pipeline: For numerical features, missing values are imputed using the mean of the column values, and the features are scaled using standardization.\n",
    "\n",
    "Categorical Pipeline: For categorical features, missing values are imputed using the most frequent value (mode) of the column, and one-hot encoding is applied to convert categorical features into binary columns.\n",
    "\n",
    "Column Transformer: The ColumnTransformer combines the numerical and categorical pipelines into a single preprocessing step.\n",
    "\n",
    "Random Forest Classifier: A Random Forest Classifier is used as the final model to make predictions.\n",
    "\n",
    "Model Evaluation: The accuracy of the model is evaluated on the test dataset.\n",
    "\n",
    "Possible Improvements:\n",
    "\n",
    "Hyperparameter tuning for the Random Forest Classifier to optimize its performance.\n",
    "Consider other feature selection methods and evaluate their impact on model performance.\n",
    "Experiment with different imputation strategies and scaling methods to find the most suitable for your data.\n",
    "Consider additional preprocessing steps, such as feature engineering or dimensionality reduction, depending on the nature of your data.\n",
    "This pipeline can serve as a starting point, and you can further refine and expand it based on the specifics of your dataset and the problem you are solving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58829e2-7741-4280-8337-d5e7e3ca3286",
   "metadata": {},
   "source": [
    "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
    "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d85a9a8-94ca-411a-8acd-612c5dc0d8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Voting Classifier: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Step 3: Create individual pipelines for Random Forest and Logistic Regression\n",
    "random_forest_pipeline = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "logistic_regression_pipeline = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Step 4: Create a Voting Classifier that combines Random Forest and Logistic Regression\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('random_forest', random_forest_pipeline),\n",
    "        ('logistic_regression', logistic_regression_pipeline)\n",
    "    ],\n",
    "    voting='soft'  # 'soft' for probability-based voting, 'hard' for majority voting\n",
    ")\n",
    "\n",
    "# Step 5: Train the pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the accuracy\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Voting Classifier: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0123bba-98e1-43a1-b04a-988244c95275",
   "metadata": {},
   "source": [
    "To build a pipeline that includes both a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions on the Iris dataset, you can follow these steps:\n",
    "\n",
    "Import necessary libraries.\n",
    "Load the Iris dataset.\n",
    "Create individual pipelines for the Random Forest Classifier and Logistic Regression Classifier.\n",
    "Create a Voting Classifier that combines the two classifiers.\n",
    "Train the pipeline on the Iris dataset.\n",
    "Evaluate the accuracy of the Voting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df17a3b-11b1-4ee3-bd4a-0d0b21577a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
