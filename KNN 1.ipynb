{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3096e5-31c6-494a-ba25-b3b1c351be37",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a4d33-2e75-4004-a118-43f35cc66973",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbor (KNN) algorithm is a popular machine learning technique used for classification and regression tasks. It relies on the idea that similar data points tend to have similar labels or values.\n",
    "During the training phase, the KNN algorithm stores the entire training dataset as a reference. When making predictions, it calculates the distance between the input data point and all the training examples, using a chosen distance metric such as Euclidean distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5aef78-8b89-4315-871e-797cb68e4aa0",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4134e7-135d-441e-b31e-d7d757a21bdc",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbor (KNN) algorithm is a popular machine learning technique used for classification and regression tasks. It relies on the idea that similar data points tend to have similar labels or values.\n",
    "During the training phase, the KNN algorithm stores the entire training dataset as a reference. When making predictions, it calculates the distance between the input data point and all the training examples, using a chosen distance metric such as Euclidean distance.\n",
    "Next, the algorithm identifies the K nearest neighbors to the input data point based on their distances. In the case of classification, the algorithm assigns the most common class label among the K neighbors as the predicted label for the input data point. For regression, it calculates the average or weighted average of the target values of the K neighbors to predict the value for the input data point.\n",
    "So first we will study about KNN classification problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fdd06c-258e-4d71-af31-f4d3666f30d6",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaac159-87db-4da0-84c8-0ff8550b06aa",
   "metadata": {},
   "source": [
    "In the case of classification, the algorithm assigns the most common class label among the K neighbors as the predicted label for the input data point. For regression, it calculates the average or weighted average of the target values of the K neighbors to predict the value for the input data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8669ee-ea49-4534-a668-11e7cd96f7c9",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c890115-67c0-42c8-992b-89b41ae22bf1",
   "metadata": {},
   "source": [
    "\tFirst of all we have to initialize the K value. The value of K is always greater than 0. It can either be 1, 2, 3, 4, 5…… This is a hyperparameter and can be changed according the problem statement. Here we will let K=5\n",
    "\tThen we find the K nearest neighbor from the test data. Here the value of K is 5 so we will find 5 data points nearest to the test data point.\n",
    "\tNow we will see how many data points from the nearest ones are of 0 category and 1 category. The category with majority of data points nearest to the new data point will be output category for that test data point. In this case we can see the 1 category has more data points near to the test data points than the 0 category. So the output category for this test data point will be 1.\n",
    "\tThe distance between the data points are calculated by using different test metrics. Generally there are 2 distance metrics used:\n",
    "\tEuclidian Distance: For 2d data points with coordinates (x_1,y_1) for first data point and (x_2,y_2) for the other. Euclidian distance formula will be:\n",
    "Distance=√((x_2-x_1 )^2+(y_2-y_1 )^2 )\n",
    "For 3d data points with coordinates (x_1,y_1,z_1) for first data point and (x_2,y_2,z_2) for the second data point. The Euclidian Distance formula will be:\n",
    "Distance=√((x_2-x_1 )^2+(y_2-y_1 )^2+(z_2-z_1 )^2 )\n",
    "The Euclidian distance metric uses Pythagoras theorem for calculating the distance.\n",
    "\tManhattan Distance: This distance is also known as taxicab distance or city block distance that is because the way this distance is calculated. The distance between two points is the sum of the absolute differences of their Cartesian coordinates. Manhattan distance is calculated by formula:\n",
    "D(x,y)=∑_(i=1)^n▒|x_i-y_i | \n",
    "We use both the distance metrics simultaneously and the one which gives higher accuracy is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4794037a-2bbc-44ef-a301-47cc55bda637",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f9bcc-f60a-4a0c-897f-9db12f892c4b",
   "metadata": {},
   "source": [
    "The number of features in a dataset is considered as dimensions for example if a dataset has 2 features then it is 2 dimensional or if it has 3 features then it is 3 dimensional and so on. A dataset can contain many features but not all of them are necessary for the output prediction or we can say only few important features affect the accuracy of the model prediction. For instance in a dataset of house price prediction there are 500 datasets. If we select 3 or 6 or 15 most important features, the accuracy of the model will keep increasing as we increase the number of features for model prediction. But if we increase number of features for model prediction to 50, there is high chance of model becoming overfit as there might be some unnecessary features in it that are not that much important for model prediction and eventually they will decrease the accuracy of the model.\n",
    "This is known as curse of dimensionality in which we keep increasing the number of features for model building and initially we have good results but eventually the model becomes overfit due to unnecessary features in it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1131ba-5885-4f5c-98f9-42d88e9249cd",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75fa60-b5d9-4b5d-8017-84965f6a80e3",
   "metadata": {},
   "source": [
    "i.\tFeature Selection: In this method we select the most important features for solving the problem statement.\n",
    "ii.\tDimensionality Reduction: In this method we not only select the most important features but also extract important information from other features as well and then form new features using them. Basically here we are doing feature extraction. For example suppose we have 500 features in a dataset, we can reduce these 500 features to 20 features by extracting important information from all the features. PCA is a method of Dimensionality Reduction which help in the feature extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371cddb-3996-46f6-9f68-3aba6ecba342",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d4899-8927-4eb0-8a00-2b814651f331",
   "metadata": {},
   "source": [
    "Let us consider we have a dataset with input features price and size and an output feature y which are continuous values. For each input data points, there is an output which is a continuous value. If we plot the data points of y on a graph it would look like this as shown here. This is the training dataset. We will use this data to train the model. The orange point shown in the graph is new data. We have to predict the continuous output value of this new data using the \n",
    "Let us consider we have a dataset with input features f_1 and f_2 and an output feature y which can either be of binary category or of multiclass category. Here in this case y is of binary category say 0 and 1. For each input data points, there is an output which is either 1 or 0. If we plot the data points of y on a graph it would look like this as shown here. The white points are of 0 category and red ones are of 1 category. This is the training dataset. We will use this data to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf24726-a06e-4ce9-a6e1-93a5b82a14ed",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7916f7d8-d2b4-47bc-af84-8ae8d221c72a",
   "metadata": {},
   "source": [
    "There is a problem in KNN algorithm. As there could be hundreds or thousands or sometimes even millions of data points in the data set. Whenever there is a new data point then its distance is calculated from all the data points and then using the K value the top nearest points are identified, this technique is known as brute search. So time complexity becomes very high. Therefore we use variants of KNN to overcome this problem\n",
    "Generally we use 2 variants:\n",
    "\tK Dimension (KD) tree\n",
    "Ball tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d114b8e-0e87-46fb-bbb4-f1cf36bd4dee",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec97418-823e-4e3b-a481-4c16ce156bbe",
   "metadata": {},
   "source": [
    "\tEuclidian Distance: For 2d data points with coordinates (x_1,y_1) for first data point and (x_2,y_2) for the other. Euclidian distance formula will be:\n",
    "Distance=√((x_2-x_1 )^2+(y_2-y_1 )^2 )\n",
    "For 3d data points with coordinates (x_1,y_1,z_1) for first data point and (x_2,y_2,z_2) for the second data point. The Euclidian Distance formula will be:\n",
    "Distance=√((x_2-x_1 )^2+(y_2-y_1 )^2+(z_2-z_1 )^2 )\n",
    "The Euclidian distance metric uses Pythagoras theorem for calculating the distance.\n",
    "\tManhattan Distance: This distance is also known as taxicab distance or city block distance that is because the way this distance is calculated. The distance between two points is the sum of the absolute differences of their Cartesian coordinates. Manhattan distance is calculated by formula:\n",
    "D(x,y)=∑_(i=1)^n▒|x_i-y_i | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8157e9-b5bb-48f7-a2f7-153ca15e3873",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346116de-8d84-412e-908e-9fdcb86bb2b0",
   "metadata": {},
   "source": [
    "There is a problem in KNN algorithm. As there could be hundreds or thousands or sometimes even millions of data points in the data set. Whenever there is a new data point then its distance is calculated from all the data points and then using the K value the top nearest points are identified, this technique is known as brute search. So time complexity becomes very high. Therefore we use variants of KNN to overcome this problem\n",
    "Generally we use 2 variants:\n",
    "\tK Dimension (KD) tree\n",
    "\tBall tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c1927-7a3d-4117-92a9-5b16cab0ee7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
